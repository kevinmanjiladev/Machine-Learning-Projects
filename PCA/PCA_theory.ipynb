{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f27758",
   "metadata": {},
   "source": [
    "PCA-Principal Component Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5bef3b",
   "metadata": {},
   "source": [
    "What is PCA in Machine Learning? (VERY SIMPLE)\n",
    "\n",
    "    PCA = Principal Component Analysis\n",
    "    PCA is a dimensionality reduction technique\n",
    "    PCA creates new features that capture the MOST information from the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f985f",
   "metadata": {},
   "source": [
    "ğŸ¯ WHY do we use PCA?\n",
    "---\n",
    "Imagine your dataset has many features (10, 20, 100, 1000â€¦).\n",
    "\n",
    "Some features:\n",
    "\n",
    "    are useless\n",
    "\n",
    "    repeat the same information\n",
    "\n",
    "    add noise\n",
    "\n",
    "PCA helps by:\n",
    "\n",
    "    âœ”ï¸ Compressing data\n",
    "    âœ”ï¸ Keeping most useful information\n",
    "    âœ”ï¸ Reducing number of features\n",
    "    âœ”ï¸ Making data easier to visualize (2D or 3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80edcf",
   "metadata": {},
   "source": [
    "ğŸŒˆ INTUITION: What PCA actually DOES\n",
    "----\n",
    "PCA looks at data and tries to find:\n",
    "\n",
    "    PC1 = direction in which data spreads the MOST\n",
    "    PC2 = direction of second most spread\n",
    "\n",
    "(PC1 âŸ‚ PC2 â†’ perpendicular)\n",
    "\n",
    "Then it rotates the data to these new directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801dcc6",
   "metadata": {},
   "source": [
    "ğŸ“Œ Example (Simple)\n",
    "---\n",
    "Letâ€™s say original features are:\n",
    "\n",
    "    Sepal length\n",
    "\n",
    "    Sepal width\n",
    "\n",
    "But these two features might be correlated.\n",
    "\n",
    "PCA finds new axes:\n",
    "\n",
    "    PC1 (combination of length + width)\n",
    "\n",
    "    PC2 (perpendicular combination)\n",
    "\n",
    "These new axes capture most of the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4bfa5",
   "metadata": {},
   "source": [
    "ğŸ§  Why PCA reduces dimensions?\n",
    "---\n",
    "If PC1 captures 90% of the variation,\n",
    "and PC2 captures only 10%,\n",
    "\n",
    "You can throw away PC2.\n",
    "\n",
    "So instead of 2 features â†’ you keep only 1 feature.\n",
    "\n",
    "This is dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fb833",
   "metadata": {},
   "source": [
    "ğŸ“Š What PCA gives you?\n",
    "---\n",
    "PCA outputs:\n",
    "| Component | What it means          |\n",
    "| --------- | ---------------------- |\n",
    "| PC1       | Most important pattern |\n",
    "| PC2       | Second most important  |\n",
    "| PC3       | Third most important   |\n",
    "| â€¦         | â€¦                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb7aa7",
   "metadata": {},
   "source": [
    "ğŸš€ Why PCA is used in ML?\n",
    "---\n",
    "    âœ”ï¸ To speed up training\n",
    "    âœ”ï¸ Remove noise\n",
    "    âœ”ï¸ Reduce overfitting\n",
    "    âœ”ï¸ Visualize data in 2D\n",
    "    âœ”ï¸ Handle correlated features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
