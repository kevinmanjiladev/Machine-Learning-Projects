{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c64a9ff",
   "metadata": {},
   "source": [
    "# üå≥ **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "**Goal:** Reduce **variance** (i.e., avoid overfitting).\n",
    "**Idea:** Train **many models independently** on **different random samples** of the data ‚Üí then **combine (average/vote)** their results.\n",
    "\n",
    "### üí° How Bagging Works\n",
    "\n",
    "1. Take many **random samples WITH replacement** from the dataset.\n",
    "2. Train a **separate model** on each sample.\n",
    "3. Combine all models‚Äô predictions:\n",
    "\n",
    "   * Classification ‚Üí **Majority voting**\n",
    "   * Regression ‚Üí **Averaging**\n",
    "\n",
    "### ‚≠ê Example Algorithms\n",
    "\n",
    "* **Random Forest** ‚Üí most famous bagging method\n",
    "* Bagged Decision Trees\n",
    "\n",
    "### üß† Intuition (Like explaining to a child)\n",
    "\n",
    "Imagine you ask **10 friends** to guess your age.\n",
    "Each guesses differently, but the **average guess** is usually correct ‚Üí that‚Äôs bagging!\n",
    "\n",
    "---\n",
    "\n",
    "# üî• **Boosting**\n",
    "\n",
    "**Goal:** Reduce **bias AND variance** by making weak models stronger.\n",
    "**Idea:** Train models **one after another**, each new model **fixes errors** made by the previous ones.\n",
    "\n",
    "### üí° How Boosting Works\n",
    "\n",
    "1. Train a weak model (like a small tree).\n",
    "2. See which samples are misclassified ‚Üí give them **more weight**.\n",
    "3. Train next model focusing on **difficult points**.\n",
    "4. Combine all models‚Äô predictions ‚Üí weighted sum/vote.\n",
    "\n",
    "### ‚≠ê Example Algorithms\n",
    "\n",
    "* **AdaBoost**\n",
    "* **Gradient Boosting Machine (GBM)**\n",
    "* **XGBoost**\n",
    "* **LightGBM**\n",
    "* **CatBoost**\n",
    "\n",
    "### üß† Intuition (Child-like explanation)\n",
    "\n",
    "Imagine a student taking practice exams.\n",
    "\n",
    "First exam: many mistakes.\n",
    "\n",
    "Second exam: they focus only on the **questions they got wrong**.\n",
    "\n",
    "Third exam: again focus on remaining mistakes.\n",
    "\n",
    "By the end, they become an expert ‚Üí that‚Äôs boosting!\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ **Bagging vs Boosting (easy table)**\n",
    "\n",
    "| Feature         | Bagging                  | Boosting                          |\n",
    "| --------------- | ------------------------ | --------------------------------- |\n",
    "| Training style  | **Parallel**             | **Sequential**                    |\n",
    "| Focus           | Reduce **variance**      | Reduce **bias + variance**        |\n",
    "| Handles errors? | Treat all models equally | New model fixes earlier errors    |\n",
    "| Risk            | **Less overfitting**     | **Can overfit** if not controlled |\n",
    "| Famous example  | Random Forest            | XGBoost                           |\n",
    "\n",
    "---\n",
    "\n",
    "# üßµ **In one line**\n",
    "\n",
    "* **Bagging = many independent models ‚Üí vote**\n",
    "* **Boosting = series of models ‚Üí each corrects previous mistakes**\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
